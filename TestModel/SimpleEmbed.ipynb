{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973f64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8d7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/Quant/project/\"\n",
    "targetpath =\"TRAIN.csv\"\n",
    "valpath = \"val.csv\"\n",
    "testpath =\"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20b05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(path+valpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c992b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>row_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22_58</td>\n",
       "      <td>22</td>\n",
       "      <td>58</td>\n",
       "      <td>0.600911</td>\n",
       "      <td>1.081210</td>\n",
       "      <td>-1.635753</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>-0.654596</td>\n",
       "      <td>2.116271</td>\n",
       "      <td>...</td>\n",
       "      <td>1.829991</td>\n",
       "      <td>1.225630</td>\n",
       "      <td>-0.133157</td>\n",
       "      <td>0.338368</td>\n",
       "      <td>-1.329777</td>\n",
       "      <td>0.458489</td>\n",
       "      <td>-1.171452</td>\n",
       "      <td>-1.116468</td>\n",
       "      <td>-1.403561</td>\n",
       "      <td>1.496008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22_59</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>-0.148273</td>\n",
       "      <td>-1.107688</td>\n",
       "      <td>-0.762889</td>\n",
       "      <td>0.759441</td>\n",
       "      <td>-0.661708</td>\n",
       "      <td>0.614619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329355</td>\n",
       "      <td>1.225630</td>\n",
       "      <td>0.335490</td>\n",
       "      <td>0.338368</td>\n",
       "      <td>0.752005</td>\n",
       "      <td>0.366660</td>\n",
       "      <td>-1.171452</td>\n",
       "      <td>0.843250</td>\n",
       "      <td>0.520654</td>\n",
       "      <td>0.824550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22_61</td>\n",
       "      <td>22</td>\n",
       "      <td>61</td>\n",
       "      <td>-0.595294</td>\n",
       "      <td>0.776524</td>\n",
       "      <td>0.546407</td>\n",
       "      <td>0.045971</td>\n",
       "      <td>-0.606577</td>\n",
       "      <td>-0.313791</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.076004</td>\n",
       "      <td>-0.815907</td>\n",
       "      <td>-0.349123</td>\n",
       "      <td>0.338368</td>\n",
       "      <td>0.752005</td>\n",
       "      <td>-0.404179</td>\n",
       "      <td>0.084959</td>\n",
       "      <td>-0.250347</td>\n",
       "      <td>-0.819772</td>\n",
       "      <td>-0.118027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>22_62</td>\n",
       "      <td>22</td>\n",
       "      <td>62</td>\n",
       "      <td>-0.983207</td>\n",
       "      <td>-0.227873</td>\n",
       "      <td>0.400930</td>\n",
       "      <td>-1.453166</td>\n",
       "      <td>0.601058</td>\n",
       "      <td>4.339244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840153</td>\n",
       "      <td>-0.815907</td>\n",
       "      <td>-0.667887</td>\n",
       "      <td>0.338368</td>\n",
       "      <td>-1.329777</td>\n",
       "      <td>3.346428</td>\n",
       "      <td>-1.171452</td>\n",
       "      <td>-0.388681</td>\n",
       "      <td>1.211371</td>\n",
       "      <td>4.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>22_63</td>\n",
       "      <td>22</td>\n",
       "      <td>63</td>\n",
       "      <td>-0.229776</td>\n",
       "      <td>-0.004463</td>\n",
       "      <td>-0.035502</td>\n",
       "      <td>2.124675</td>\n",
       "      <td>-0.658155</td>\n",
       "      <td>-0.573081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808395</td>\n",
       "      <td>-0.815907</td>\n",
       "      <td>-0.425994</td>\n",
       "      <td>0.338368</td>\n",
       "      <td>0.752005</td>\n",
       "      <td>-0.606159</td>\n",
       "      <td>-1.171452</td>\n",
       "      <td>0.717530</td>\n",
       "      <td>-1.274164</td>\n",
       "      <td>-0.703374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>24995</td>\n",
       "      <td>33_213</td>\n",
       "      <td>33</td>\n",
       "      <td>213</td>\n",
       "      <td>-1.414616</td>\n",
       "      <td>0.833006</td>\n",
       "      <td>-1.078762</td>\n",
       "      <td>-0.972510</td>\n",
       "      <td>2.457652</td>\n",
       "      <td>-0.235570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120991</td>\n",
       "      <td>-0.657001</td>\n",
       "      <td>-0.324576</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>0.825183</td>\n",
       "      <td>-0.382438</td>\n",
       "      <td>-0.283733</td>\n",
       "      <td>-0.444719</td>\n",
       "      <td>1.520904</td>\n",
       "      <td>-0.176580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>24996</td>\n",
       "      <td>33_214</td>\n",
       "      <td>33</td>\n",
       "      <td>214</td>\n",
       "      <td>-0.006400</td>\n",
       "      <td>-0.584242</td>\n",
       "      <td>0.633364</td>\n",
       "      <td>0.188913</td>\n",
       "      <td>-0.773373</td>\n",
       "      <td>-0.483320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446051</td>\n",
       "      <td>-0.657001</td>\n",
       "      <td>-0.452254</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>0.825183</td>\n",
       "      <td>-0.401591</td>\n",
       "      <td>-0.283733</td>\n",
       "      <td>-0.329566</td>\n",
       "      <td>0.846726</td>\n",
       "      <td>-0.484327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>24997</td>\n",
       "      <td>33_216</td>\n",
       "      <td>33</td>\n",
       "      <td>216</td>\n",
       "      <td>-0.275792</td>\n",
       "      <td>0.087031</td>\n",
       "      <td>1.204072</td>\n",
       "      <td>0.267470</td>\n",
       "      <td>-0.389980</td>\n",
       "      <td>-0.500376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619648</td>\n",
       "      <td>-0.657001</td>\n",
       "      <td>-1.341064</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>-1.211851</td>\n",
       "      <td>-0.528420</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>-0.913289</td>\n",
       "      <td>-0.411646</td>\n",
       "      <td>-0.506684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>24998</td>\n",
       "      <td>33_217</td>\n",
       "      <td>33</td>\n",
       "      <td>217</td>\n",
       "      <td>-0.934072</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>-1.649470</td>\n",
       "      <td>-1.010666</td>\n",
       "      <td>1.146295</td>\n",
       "      <td>-0.087223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394762</td>\n",
       "      <td>-0.657001</td>\n",
       "      <td>-0.934546</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>0.825183</td>\n",
       "      <td>-0.255277</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>-0.845721</td>\n",
       "      <td>-0.133167</td>\n",
       "      <td>-0.050718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>24999</td>\n",
       "      <td>33_218</td>\n",
       "      <td>33</td>\n",
       "      <td>218</td>\n",
       "      <td>-0.925161</td>\n",
       "      <td>-0.320688</td>\n",
       "      <td>1.917458</td>\n",
       "      <td>-0.796179</td>\n",
       "      <td>1.781502</td>\n",
       "      <td>-0.398205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086799</td>\n",
       "      <td>-0.657001</td>\n",
       "      <td>0.119822</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>0.825183</td>\n",
       "      <td>-0.417409</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>-1.102599</td>\n",
       "      <td>1.377287</td>\n",
       "      <td>-0.373196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  row_id  time_id  investment_id    target       f_0  \\\n",
       "0               0   22_58       22             58  0.600911  1.081210   \n",
       "1               1   22_59       22             59 -0.148273 -1.107688   \n",
       "2               2   22_61       22             61 -0.595294  0.776524   \n",
       "3               3   22_62       22             62 -0.983207 -0.227873   \n",
       "4               4   22_63       22             63 -0.229776 -0.004463   \n",
       "...           ...     ...      ...            ...       ...       ...   \n",
       "24995       24995  33_213       33            213 -1.414616  0.833006   \n",
       "24996       24996  33_214       33            214 -0.006400 -0.584242   \n",
       "24997       24997  33_216       33            216 -0.275792  0.087031   \n",
       "24998       24998  33_217       33            217 -0.934072  0.528163   \n",
       "24999       24999  33_218       33            218 -0.925161 -0.320688   \n",
       "\n",
       "            f_1       f_2       f_3       f_4  ...     f_290     f_291  \\\n",
       "0     -1.635753  0.082000 -0.654596  2.116271  ...  1.829991  1.225630   \n",
       "1     -0.762889  0.759441 -0.661708  0.614619  ...  0.329355  1.225630   \n",
       "2      0.546407  0.045971 -0.606577 -0.313791  ... -1.076004 -0.815907   \n",
       "3      0.400930 -1.453166  0.601058  4.339244  ...  0.840153 -0.815907   \n",
       "4     -0.035502  2.124675 -0.658155 -0.573081  ...  0.808395 -0.815907   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "24995 -1.078762 -0.972510  2.457652 -0.235570  ... -0.120991 -0.657001   \n",
       "24996  0.633364  0.188913 -0.773373 -0.483320  ...  0.446051 -0.657001   \n",
       "24997  1.204072  0.267470 -0.389980 -0.500376  ... -0.619648 -0.657001   \n",
       "24998 -1.649470 -1.010666  1.146295 -0.087223  ...  0.394762 -0.657001   \n",
       "24999  1.917458 -0.796179  1.781502 -0.398205  ... -0.086799 -0.657001   \n",
       "\n",
       "          f_292     f_293     f_294     f_295     f_296     f_297     f_298  \\\n",
       "0     -0.133157  0.338368 -1.329777  0.458489 -1.171452 -1.116468 -1.403561   \n",
       "1      0.335490  0.338368  0.752005  0.366660 -1.171452  0.843250  0.520654   \n",
       "2     -0.349123  0.338368  0.752005 -0.404179  0.084959 -0.250347 -0.819772   \n",
       "3     -0.667887  0.338368 -1.329777  3.346428 -1.171452 -0.388681  1.211371   \n",
       "4     -0.425994  0.338368  0.752005 -0.606159 -1.171452  0.717530 -1.274164   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24995 -0.324576  0.173576  0.825183 -0.382438 -0.283733 -0.444719  1.520904   \n",
       "24996 -0.452254  0.173576  0.825183 -0.401591 -0.283733 -0.329566  0.846726   \n",
       "24997 -1.341064  0.173576 -1.211851 -0.528420  0.992241 -0.913289 -0.411646   \n",
       "24998 -0.934546  0.173576  0.825183 -0.255277  0.992241 -0.845721 -0.133167   \n",
       "24999  0.119822  0.173576  0.825183 -0.417409  0.992241 -1.102599  1.377287   \n",
       "\n",
       "          f_299  \n",
       "0      1.496008  \n",
       "1      0.824550  \n",
       "2     -0.118027  \n",
       "3      4.564400  \n",
       "4     -0.703374  \n",
       "...         ...  \n",
       "24995 -0.176580  \n",
       "24996 -0.484327  \n",
       "24997 -0.506684  \n",
       "24998 -0.050718  \n",
       "24999 -0.373196  \n",
       "\n",
       "[25000 rows x 305 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee31a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#两方面去考虑attention,首先,可以忽视事件数据,将所有Id一样的人聚集在一起,将他们视作一个sequence,将他们的target作为另一个,观察两个sequence的关系\n",
    "#其次,可以以同一个时域做Attention,将这个时域中的数据作为一个切面sequence,然后跟他们的结果做相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebc158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDataFromKaggle():\n",
    "    print(\"successful get date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab96e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMaxFeature(x):\n",
    "    id_size = x[\"investment_id\"].nunique()\n",
    "    time_id_size = x[\"time_id\"].nunique()\n",
    "    return id_size,time_id_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4562374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ubiquant(Dataset):\n",
    "    def __init__(self,datapath,train_data = True,get_time_data = True):\n",
    "        super(Ubiquant,self).__init__()\n",
    "        self.get_time = get_time_data\n",
    "        if(train_data):\n",
    "            self.data = pd.read_csv(datapath)\n",
    "            self.id_size,self.time_size = GetMaxFeature(self.data)\n",
    "            if(get_time_data):#get an x,y base on time frame\n",
    "                self.y = self.data.loc[:,\"target\"]\n",
    "                self.x = self.data.drop(columns=[\"Unnamed: 0\",\"row_id\",\"investment_id\",\"target\"])\n",
    "                self.x.loc[:,\"time_id\"] -=self.x.loc[0,\"time_id\"]\n",
    "                self.id = self.x[[\"time_id\"]]\n",
    "                self.x = self.x.drop(columns=[\"time_id\"])\n",
    "            else:#get (x,y) base on investment_id\n",
    "                self.data = self.data.sort_values(by=[\"investment_id\"])\n",
    "                self.y = self.data.loc[:,\"target\"]\n",
    "                self.x = self.data.drop(columns=[\"Unnamed: 0\",\"row_id\",\"time_id\",\"target\"])\n",
    "                self.id = self.x[[\"investment_id\"]]\n",
    "                self.x = self.x.drop(columns=[\"investment_id\"])\n",
    "        \n",
    "        self.id =torch.LongTensor(self.id.to_numpy())\n",
    "        self.x = torch.FloatTensor(self.x.to_numpy())\n",
    "        self.y = torch.FloatTensor(self.y.to_numpy())\n",
    "        \n",
    "        print(f\"succesfully load data with shape x : {self.x.shape} | shape y :{self.y.shape} | get time data is: {get_time_data}\")\n",
    "        \n",
    "                \n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.id[index],self.x[index],self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def GetSize(self):\n",
    "        if(get_time_data):\n",
    "            return self.time_size\n",
    "        else:\n",
    "            return self.id_size\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fbfea802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(get_time_data = True):\n",
    "    trainSet =Ubiquant(path+targetpath, get_time_data = get_time_data)\n",
    "    valSet =Ubiquant(path+valpath, get_time_data = get_time_data)\n",
    "    testSet =Ubiquant(path+testpath, get_time_data = get_time_data)\n",
    "    trainLoader = DataLoader(trainSet,batch_size = config[\"batch_size\"],shuffle =True,drop_last=True)#,num_workers=2)\n",
    "    valLoader = DataLoader(valSet,batch_size = config[\"batch_size\"],shuffle =True,drop_last=True)#,num_workers=2)\n",
    "    testLoader = DataLoader(testSet,batch_size = config[\"batch_size\"],shuffle =False,drop_last=False)\n",
    "    return trainLoader,valLoader,testLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0838ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self,input_feature = 311):\n",
    "        super(SimpleMLP,self).__init__()\n",
    "        \n",
    "        self.id_embedding = nn.Embedding(10000,11)\n",
    "        \n",
    "        self.MLP =nn.Sequential(\n",
    "            nn.Linear(input_feature,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128,8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(8,1)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,_id,f_features):\n",
    "        invest_embedding = self.id_embedding(_id).squeeze(dim=1)\n",
    "        _input =torch.cat((invest_embedding,f_features),axis =-1)\n",
    "        output = self.MLP(_input)\n",
    "        return output\n",
    "\n",
    "    def cal_Loss(self,y_hat,y):\n",
    "        return self.criterion(y_hat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6fdf7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(y,target):\n",
    "    y,target = y.reshape(-1),target.reshape(-1)\n",
    "    ymean,targetmean = torch.mean(y),torch.mean(target)\n",
    "    \n",
    "    vy = y-ymean\n",
    "    vt = target-targetmean\n",
    "    \n",
    "    corr = torch.sum(vy*vt)/(torch.sqrt(torch.sum(vy**2))* torch.sqrt(torch.sum(vt**2)))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "111db94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,trainLoader,valLoader, optimizer, scheduler, config):\n",
    "    best_loss =1000\n",
    "    epochs =config[\"epoch\"]\n",
    "    for epoch in range(epochs):\n",
    "        batch_bar = tqdm(total = len(trainLoader),dynamic_ncols = True,leave =False,position = 0,desc = \"train\")\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        for i,(x1,x2,y) in enumerate(trainLoader):\n",
    "            optimizer.zero_grad()\n",
    "            x1,x2,y = x1.cuda(),x2.cuda(),y.cuda()\n",
    "            y_hat = model(x1,x2)\n",
    "            y_hat = y_hat.reshape(-1,1)\n",
    "            y =y.reshape(-1,1)\n",
    "            loss = model.cal_Loss(y_hat,y)\n",
    "            train_total_loss+=float(loss.cpu())\n",
    "            batch_bar.set_postfix(\n",
    "                loss=\"{:.04f}\".format(float(train_total_loss / (i + 1))),\n",
    "                lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            batch_bar.update()\n",
    "        batch_bar.close()\n",
    "        \n",
    "        val_total_loss = 0\n",
    "        val_total_corr = 0\n",
    "        batch_bar = tqdm(total = len(valLoader),dynamic_ncols = True,leave =False,position = 0,desc = \"val\")\n",
    "        model.eval()\n",
    "        for i,(x1,x2,y) in enumerate(valLoader):\n",
    "            with torch.no_grad():\n",
    "                x1,x2,y = x1.cuda(),x2.cuda(),y.cuda()\n",
    "                y_hat = model(x1,x2)\n",
    "                y_hat = y_hat.reshape(-1,1)\n",
    "                y =y.reshape(-1,1)\n",
    "                loss = model.cal_Loss(y_hat,y)\n",
    "                \n",
    "\n",
    "                corr = get_corr(y_hat,y)\n",
    "                val_total_corr+=corr\n",
    "               \n",
    "                val_total_loss+=loss\n",
    "            batch_bar.set_postfix(\n",
    "                loss=\"{:.04f}\".format(float(val_total_loss / (i + 1))),\n",
    "                \n",
    "                corr=\"{:.04f}\".format(float(val_total_corr / (i + 1))),\n",
    "             )\n",
    "            batch_bar.update()\n",
    "        batch_bar.close()\n",
    "        val_loss = float(val_total_loss/len(valLoader))\n",
    "        if(val_loss<best_loss):\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(),config[\"store_path\"])\n",
    "            print(\"successfully save model\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: train loss {float(train_total_loss / len(trainLoader)):0.04f}, Learning Rate {optimizer.param_groups[0]['lr']:0.04f}, val loss{float(val_total_loss/len(valLoader)):0.04f}, corr {float(val_total_corr / (i + 1)):0.04f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "633f0a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    #set all parameter\n",
    "    trainLoader,valLoader,testLoader = LoadData(get_time_data = False)\n",
    "    model = SimpleMLP().cuda()\n",
    "    model.train()\n",
    "    optimizer = getattr(torch.optim,config[\"optimz\"])(model.parameters(),lr =config[\"learning_rate\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(trainLoader) * config[\"epoch\"]))\n",
    "    \n",
    "    #params\n",
    "    num_para =0\n",
    "    for p in model.parameters():\n",
    "        num_para+=p.numel()\n",
    "    print(f'Number of params : {num_para}')\n",
    "    \n",
    "    train(model,trainLoader,valLoader,optimizer,scheduler,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5fd12bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"epoch\" : 50,\"batch_size\":12800,\"learning_rate\":0.1,\"optimz\":\"Adam\",\"store_path\":\"C:/Quant/project/MLP.pth\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29b82d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succesfully load data with shape x : torch.Size([50000, 300]) | shape y :torch.Size([50000]) | get time data is: False\n",
      "succesfully load data with shape x : torch.Size([25000, 300]) | shape y :torch.Size([25000]) | get time data is: False\n",
      "succesfully load data with shape x : torch.Size([25000, 300]) | shape y :torch.Size([25000]) | get time data is: False\n",
      "Number of params : 470865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 1/50: train loss 1.0170, Learning Rate 0.0999, val loss763.7325, corr 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 2/50: train loss 0.9054, Learning Rate 0.0996, val loss547.5977, corr 0.0379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              00]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 3/50: train loss 0.9019, Learning Rate 0.0991, val loss31.4973, corr -0.0502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 4/50: train loss 0.8963, Learning Rate 0.0984, val loss4.0075, corr -0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 5/50: train loss 0.9017, Learning Rate 0.0976, val loss0.8772, corr 0.0289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: train loss 0.8985, Learning Rate 0.0965, val loss0.9576, corr 0.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 7/50: train loss 0.8950, Learning Rate 0.0952, val loss0.8519, corr 0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: train loss 0.8945, Learning Rate 0.0938, val loss0.8750, corr 0.0360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 9/50: train loss 0.8839, Learning Rate 0.0922, val loss0.8513, corr -0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: train loss 0.8750, Learning Rate 0.0905, val loss0.8721, corr -0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: train loss 0.8928, Learning Rate 0.0885, val loss0.8888, corr -0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: train loss 0.8829, Learning Rate 0.0864, val loss0.8857, corr -0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: train loss 0.8740, Learning Rate 0.0842, val loss0.8532, corr -0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: train loss 0.8842, Learning Rate 0.0819, val loss0.8566, corr -0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: train loss 0.8768, Learning Rate 0.0794, val loss0.8900, corr -0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: train loss 0.8823, Learning Rate 0.0768, val loss0.8806, corr -0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: train loss 0.8839, Learning Rate 0.0741, val loss0.8723, corr -0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: train loss 0.8741, Learning Rate 0.0713, val loss0.8644, corr -0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: train loss 0.8762, Learning Rate 0.0684, val loss0.8746, corr 0.0087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: train loss 0.8791, Learning Rate 0.0655, val loss0.8653, corr -0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: train loss 0.8890, Learning Rate 0.0624, val loss0.8688, corr -0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: train loss 0.8700, Learning Rate 0.0594, val loss0.8841, corr 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: train loss 0.8675, Learning Rate 0.0563, val loss0.9021, corr 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: train loss 0.8724, Learning Rate 0.0531, val loss0.8621, corr 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: train loss 0.8741, Learning Rate 0.0500, val loss0.8637, corr 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: train loss 0.8781, Learning Rate 0.0469, val loss0.8700, corr 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: train loss 0.8599, Learning Rate 0.0437, val loss0.8797, corr 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: train loss 0.8662, Learning Rate 0.0406, val loss0.8612, corr 0.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully save model\n",
      "Epoch 29/50: train loss 0.8696, Learning Rate 0.0376, val loss0.8425, corr 0.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: train loss 0.8552, Learning Rate 0.0345, val loss0.8636, corr 0.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: train loss 0.8614, Learning Rate 0.0316, val loss0.8857, corr 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: train loss 0.8523, Learning Rate 0.0287, val loss0.8676, corr 0.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: train loss 0.8547, Learning Rate 0.0259, val loss0.8888, corr 0.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: train loss 0.8599, Learning Rate 0.0232, val loss0.8902, corr 0.0414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: train loss 0.8598, Learning Rate 0.0206, val loss0.8809, corr 0.0556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: train loss 0.8525, Learning Rate 0.0181, val loss0.8994, corr 0.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: train loss 0.8472, Learning Rate 0.0158, val loss0.9015, corr 0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: train loss 0.8566, Learning Rate 0.0136, val loss0.8955, corr 0.0482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: train loss 0.8509, Learning Rate 0.0115, val loss0.9037, corr 0.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: train loss 0.8419, Learning Rate 0.0095, val loss0.8713, corr 0.0531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: train loss 0.8435, Learning Rate 0.0078, val loss0.9003, corr 0.0589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: train loss 0.8399, Learning Rate 0.0062, val loss0.8803, corr 0.0394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: train loss 0.8433, Learning Rate 0.0048, val loss0.8861, corr 0.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: train loss 0.8312, Learning Rate 0.0035, val loss0.9169, corr 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: train loss 0.8450, Learning Rate 0.0024, val loss0.8900, corr 0.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: train loss 0.8442, Learning Rate 0.0016, val loss0.8931, corr 0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: train loss 0.8472, Learning Rate 0.0009, val loss0.8941, corr 0.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: train loss 0.8414, Learning Rate 0.0004, val loss0.9127, corr 0.0553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: train loss 0.8369, Learning Rate 0.0001, val loss0.8965, corr 0.0463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: train loss 0.8447, Learning Rate 0.0000, val loss0.8795, corr 0.0586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4132532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f7768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
